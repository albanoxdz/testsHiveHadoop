<?xml version="1.0" encoding="UTF-8"?>
<testsuite name="com.bdicm.app.TestCasesSprint_01" time="39.169" tests="10" errors="1" skipped="0" failures="1">
  <properties>
    <property name="java.runtime.name" value="Java(TM) SE Runtime Environment"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-7-oracle/jre/lib/amd64"/>
    <property name="java.vm.version" value="24.76-b04"/>
    <property name="java.vm.vendor" value="Oracle Corporation"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="path.separator" value=":"/>
    <property name="guice.disable.misplaced.annotation.check" value="true"/>
    <property name="java.vm.name" value="Java HotSpot(TM) 64-Bit Server VM"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="user.country" value="US"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="user.dir" value="/home/usuario/workspace/testsHiveHadoop"/>
    <property name="java.runtime.version" value="1.7.0_76-b13"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-7-oracle/jre/lib/endorsed"/>
    <property name="os.arch" value="amd64"/>
    <property name="java.io.tmpdir" value="/tmp"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="os.name" value="Linux"/>
    <property name="classworlds.conf" value="/usr/share/maven/bin/m2.conf"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="java.class.version" value="51.0"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="os.version" value="3.13.0-48-generic"/>
    <property name="user.home" value="/root"/>
    <property name="user.timezone" value="America/Sao_Paulo"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.specification.version" value="1.7"/>
    <property name="user.name" value="root"/>
    <property name="java.class.path" value="/usr/share/maven/boot/plexus-classworlds-2.x.jar"/>
    <property name="java.vm.specification.version" value="1.7"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="java.home" value="/usr/lib/jvm/java-7-oracle/jre"/>
    <property name="sun.java.command" value="org.codehaus.plexus.classworlds.launcher.Launcher install"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="user.language" value="en"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.version" value="1.7.0_76"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-7-oracle/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="securerandom.source" value="file:/dev/./urandom"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/java-7-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-7-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-7-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-7-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-7-oracle/jre/classes"/>
    <property name="java.vendor" value="Oracle Corporation"/>
    <property name="maven.home" value="/usr/share/maven"/>
    <property name="file.separator" value="/"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="sun.cpu.isalist" value=""/>
  </properties>
  <testcase name="testQuery_01" classname="com.bdicm.app.TestCasesSprint_01" time="10.853"/>
  <testcase name="testQuery_02" classname="com.bdicm.app.TestCasesSprint_01" time="7.152"/>
  <testcase name="testQuery_03" classname="com.bdicm.app.TestCasesSprint_01" time="4.765"/>
  <testcase name="testQuery_04" classname="com.bdicm.app.TestCasesSprint_01" time="3.384"/>
  <testcase name="testQuery_05" classname="com.bdicm.app.TestCasesSprint_01" time="2.856"/>
  <testcase name="testQuery_06" classname="com.bdicm.app.TestCasesSprint_01" time="3.114"/>
  <testcase name="testQuery_07" classname="com.bdicm.app.TestCasesSprint_01" time="3.253"/>
  <testcase name="testQuery_08" classname="com.bdicm.app.TestCasesSprint_01" time="0.751">
    <error message="Failed to executeQuery Hive query SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname: Query returned non-zero code: 40000, cause: FAILED: ParseException line 1:80 cannot recognize input near &apos;cli&apos; &apos;,&apos; &apos;sales&apos; in table source&#10;" type="java.lang.IllegalStateException">java.lang.IllegalStateException: Failed to executeQuery Hive query SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname: Query returned non-zero code: 40000, cause: FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:219)
	at com.klarna.hiverunner.HiveServerContainer.executeQuery(HiveServerContainer.java:90)
	at com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:76)
	at com.bdicm.app.TestCasesSprint_01.getActual(TestCasesSprint_01.java:125)
	at com.bdicm.app.TestCasesSprint_01.testQuery_08(TestCasesSprint_01.java:194)
</error>
    <system-err><![CDATA[15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/05/25 20:39:40 INFO metastore.ObjectStore: ObjectStore, initialize called
15/05/25 20:39:40 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
15/05/25 20:39:40 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15/05/25 20:39:40 INFO metastore.ObjectStore: Initialized ObjectStore
15/05/25 20:39:40 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.12.0
15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: Shutting down the object store...
15/05/25 20:39:40 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: Metastore shutdown complete.
15/05/25 20:39:40 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15/05/25 20:39:40 INFO service.HiveServer: Putting temp output to file /tmp/junit8058007518777131183/localscratchdir/4b21b823-78a7-468c-9b6c-ba43e34c0c258063613709333529387.pipeout
15/05/25 20:39:40 INFO service.HiveServer: Running the query: SHOW TABLES
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:40 INFO parse.ParseDriver: Parsing command: SHOW TABLES
15/05/25 20:39:40 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:40 INFO ql.Driver: </PERFLOG method=parse start=1432597180969 end=1432597180970 duration=1>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:40 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:40 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597180970 end=1432597180971 duration=1>
15/05/25 20:39:40 INFO exec.ListSinkOperator: Initializing Self 241 OP
15/05/25 20:39:40 INFO exec.ListSinkOperator: Operator 241 OP initialized
15/05/25 20:39:40 INFO exec.ListSinkOperator: Initialization Done 241 OP
15/05/25 20:39:40 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
15/05/25 20:39:40 INFO ql.Driver: </PERFLOG method=compile start=1432597180969 end=1432597180972 duration=3>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:40 INFO ql.Driver: Starting command: SHOW TABLES
15/05/25 20:39:40 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597180969 end=1432597180972 duration=3>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:40 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:40 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/05/25 20:39:40 INFO metastore.ObjectStore: ObjectStore, initialize called
15/05/25 20:39:40 INFO metastore.ObjectStore: Initialized ObjectStore
15/05/25 20:39:40 INFO metastore.HiveMetaStore: 0: get_tables: db=default pat=.*
15/05/25 20:39:40 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
15/05/25 20:39:40 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/05/25 20:39:40 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597180972 end=1432597181040 duration=68>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597180972 end=1432597181041 duration=69>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597180972 end=1432597181041 duration=69>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181041 end=1432597181042 duration=1>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597180969 end=1432597181042 duration=73>
15/05/25 20:39:41 INFO builder.HiveShellTearable: Executing script: create schema ${hiveconf:my.schema}
15/05/25 20:39:41 INFO service.HiveServer: Running the query: create schema ${hiveconf:my.schema}
15/05/25 20:39:41 INFO exec.ListSinkOperator: 241 finished. closing... 
15/05/25 20:39:41 INFO exec.ListSinkOperator: 241 forwarded 0 rows
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: create schema bar
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181045 end=1432597181046 duration=1>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181047 end=1432597181047 duration=0>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181044 end=1432597181048 duration=4>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: create schema bar
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181044 end=1432597181049 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_database: Database(name:bar, description:null, locationUri:null, parameters:null)
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:bar, description:null, locationUri:null, parameters:null)	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_database: bar
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bar	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181049 end=1432597181059 duration=10>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181049 end=1432597181059 duration=10>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181048 end=1432597181059 duration=11>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181060 end=1432597181060 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181044 end=1432597181060 duration=16>
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/transactions/transactions_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/cards/cards_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/categories/categories_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/locations/locations_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/clients/clients_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/sales/sales_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit8058007518777131183/hadooptmp/products/products_table.csv
15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/clients/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/clients/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181066 end=1432597181069 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table clients position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181070 end=1432597181070 duration=0>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181066 end=1432597181071 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/clients/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181066 end=1432597181072 duration=6>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table clients
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:clients, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cli_id, type:int, comment:null), FieldSchema(name:cli_loc_id, type:int, comment:null), FieldSchema(name:cli_firstname, type:string, comment:null), FieldSchema(name:cli_lastname, type:string, comment:null), FieldSchema(name:cli_phone, type:string, comment:null), FieldSchema(name:cli_cellphone, type:string, comment:null), FieldSchema(name:cli_cpf, type:string, comment:null), FieldSchema(name:cli_gender, type:string, comment:null), FieldSchema(name:cli_email, type:string, comment:null), FieldSchema(name:cli_password, type:string, comment:null), FieldSchema(name:cli_birthdate, type:date, comment:null), FieldSchema(name:cli_rg, type:string, comment:null), FieldSchema(name:cli_income, type:string, comment:null), FieldSchema(name:cli_token, type:string, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/clients, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:clients, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cli_id, type:int, comment:null), FieldSchema(name:cli_loc_id, type:int, comment:null), FieldSchema(name:cli_firstname, type:string, comment:null), FieldSchema(name:cli_lastname, type:string, comment:null), FieldSchema(name:cli_phone, type:string, comment:null), FieldSchema(name:cli_cellphone, type:string, comment:null), FieldSchema(name:cli_cpf, type:string, comment:null), FieldSchema(name:cli_gender, type:string, comment:null), FieldSchema(name:cli_email, type:string, comment:null), FieldSchema(name:cli_password, type:string, comment:null), FieldSchema(name:cli_birthdate, type:date, comment:null), FieldSchema(name:cli_rg, type:string, comment:null), FieldSchema(name:cli_income, type:string, comment:null), FieldSchema(name:cli_token, type:string, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/clients, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181072 end=1432597181112 duration=40>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181072 end=1432597181112 duration=40>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181071 end=1432597181112 duration=41>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181112 end=1432597181112 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181066 end=1432597181113 duration=47>
15/05/25 20:39:41 INFO service.HiveServer: Running the query: 

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/transactions/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/transactions/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181114 end=1432597181118 duration=4>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table transactions position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181118 end=1432597181119 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181114 end=1432597181120 duration=6>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/transactions/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181114 end=1432597181121 duration=7>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table transactions
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:transactions, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:trans_id, type:int, comment:null), FieldSchema(name:trans_cli_id, type:int, comment:null), FieldSchema(name:trans_card_id, type:int, comment:null), FieldSchema(name:trans_loc_id, type:int, comment:null), FieldSchema(name:trans_date, type:timestamp, comment:null), FieldSchema(name:trans_value, type:double, comment:null), FieldSchema(name:trans_lat, type:double, comment:null), FieldSchema(name:trans_lon, type:double, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/transactions, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:transactions, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:trans_id, type:int, comment:null), FieldSchema(name:trans_cli_id, type:int, comment:null), FieldSchema(name:trans_card_id, type:int, comment:null), FieldSchema(name:trans_loc_id, type:int, comment:null), FieldSchema(name:trans_date, type:timestamp, comment:null), FieldSchema(name:trans_value, type:double, comment:null), FieldSchema(name:trans_lat, type:double, comment:null), FieldSchema(name:trans_lon, type:double, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/transactions, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181122 end=1432597181140 duration=18>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181121 end=1432597181140 duration=19>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181121 end=1432597181140 duration=19>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181141 end=1432597181141 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181114 end=1432597181142 duration=28>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/categories/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/categories/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181144 end=1432597181149 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table categories position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181149 end=1432597181151 duration=2>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181144 end=1432597181151 duration=7>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/categories/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181143 end=1432597181152 duration=9>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table categories
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:categories, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cat_id, type:int, comment:null), FieldSchema(name:cat_name, type:string, comment:null), FieldSchema(name:cat_description, type:string, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/categories, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:categories, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cat_id, type:int, comment:null), FieldSchema(name:cat_name, type:string, comment:null), FieldSchema(name:cat_description, type:string, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/categories, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181152 end=1432597181168 duration=16>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181152 end=1432597181168 duration=16>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181152 end=1432597181168 duration=16>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181169 end=1432597181169 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181143 end=1432597181169 duration=26>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/cards/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/cards/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181171 end=1432597181173 duration=2>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table ccards position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181173 end=1432597181174 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181170 end=1432597181175 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/cards/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181170 end=1432597181176 duration=6>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table ccards
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:ccards, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:card_id, type:int, comment:null), FieldSchema(name:card_band, type:string, comment:null), FieldSchema(name:card_number, type:string, comment:null), FieldSchema(name:cardv_month, type:int, comment:null), FieldSchema(name:cardv_year, type:int, comment:null), FieldSchema(name:card_name, type:string, comment:null), FieldSchema(name:card_lastname, type:string, comment:null), FieldSchema(name:ccard_cvv, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/cards, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:ccards, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:card_id, type:int, comment:null), FieldSchema(name:card_band, type:string, comment:null), FieldSchema(name:card_number, type:string, comment:null), FieldSchema(name:cardv_month, type:int, comment:null), FieldSchema(name:cardv_year, type:int, comment:null), FieldSchema(name:card_name, type:string, comment:null), FieldSchema(name:card_lastname, type:string, comment:null), FieldSchema(name:ccard_cvv, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/cards, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181177 end=1432597181198 duration=21>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181176 end=1432597181198 duration=22>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181175 end=1432597181198 duration=23>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181199 end=1432597181199 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181170 end=1432597181199 duration=29>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/locations/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/locations/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181201 end=1432597181203 duration=2>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table locations position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181203 end=1432597181204 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181200 end=1432597181205 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/locations/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181200 end=1432597181205 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table locations
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:locations, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:loc_id, type:int, comment:null), FieldSchema(name:loc_address, type:string, comment:null), FieldSchema(name:loc_city, type:string, comment:null), FieldSchema(name:loc_zipcode, type:string, comment:null), FieldSchema(name:loc_region, type:string, comment:null), FieldSchema(name:loc_country, type:string, comment:null), FieldSchema(name:loc_lat, type:double, comment:null), FieldSchema(name:loc_lon, type:double, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/locations, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:locations, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:loc_id, type:int, comment:null), FieldSchema(name:loc_address, type:string, comment:null), FieldSchema(name:loc_city, type:string, comment:null), FieldSchema(name:loc_zipcode, type:string, comment:null), FieldSchema(name:loc_region, type:string, comment:null), FieldSchema(name:loc_country, type:string, comment:null), FieldSchema(name:loc_lat, type:double, comment:null), FieldSchema(name:loc_lon, type:double, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/locations, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181206 end=1432597181222 duration=16>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181206 end=1432597181223 duration=17>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181205 end=1432597181223 duration=18>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181223 end=1432597181223 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181200 end=1432597181224 duration=24>
15/05/25 20:39:41 INFO service.HiveServer: Running the query: 

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/products/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/products/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181225 end=1432597181227 duration=2>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table products position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181227 end=1432597181228 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181225 end=1432597181229 duration=4>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/products/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181225 end=1432597181229 duration=4>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table products
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:products, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:prd_id, type:int, comment:null), FieldSchema(name:prd_name, type:string, comment:null), FieldSchema(name:prd_value, type:double, comment:null), FieldSchema(name:prd_descr, type:string, comment:null), FieldSchema(name:prd_category_id, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/products, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:products, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:prd_id, type:int, comment:null), FieldSchema(name:prd_name, type:string, comment:null), FieldSchema(name:prd_value, type:double, comment:null), FieldSchema(name:prd_descr, type:string, comment:null), FieldSchema(name:prd_category_id, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/products, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181230 end=1432597181253 duration=23>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181230 end=1432597181254 duration=24>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181229 end=1432597181254 duration=25>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181255 end=1432597181255 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181225 end=1432597181256 duration=31>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/sales/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/sales/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181257 end=1432597181259 duration=2>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table sales position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181260 end=1432597181260 duration=0>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181257 end=1432597181261 duration=4>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit8058007518777131183/hadooptmp/sales/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181257 end=1432597181262 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table sales
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:sales, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:sale_id, type:int, comment:null), FieldSchema(name:sale_cli_id, type:string, comment:null), FieldSchema(name:sale_value, type:double, comment:null), FieldSchema(name:sale_vol, type:int, comment:null), FieldSchema(name:sale_date, type:timestamp, comment:null), FieldSchema(name:sale_prod_id, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/sales, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:sales, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:sale_id, type:int, comment:null), FieldSchema(name:sale_cli_id, type:string, comment:null), FieldSchema(name:sale_value, type:double, comment:null), FieldSchema(name:sale_vol, type:int, comment:null), FieldSchema(name:sale_date, type:timestamp, comment:null), FieldSchema(name:sale_prod_id, type:int, comment:null)], location:/tmp/junit8058007518777131183/hadooptmp/sales, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181262 end=1432597181279 duration=17>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181262 end=1432597181279 duration=17>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181261 end=1432597181280 duration=19>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181280 end=1432597181280 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181257 end=1432597181280 duration=23>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname
NoViableAltException(10@[179:68: ( ( KW_AS )? alias= Identifier )?])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableSource(HiveParser_FromClauseParser.java:3939)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3112)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1326)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1199)
	at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:31418)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:29520)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regular_body(HiveParser.java:29428)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatement(HiveParser.java:28968)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:28762)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1238)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:938)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:977)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)
	at com.klarna.hiverunner.HiveServerContainer.executeQuery(HiveServerContainer.java:90)
	at com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:76)
	at com.bdicm.app.TestCasesSprint_01.getActual(TestCasesSprint_01.java:125)
	at com.bdicm.app.TestCasesSprint_01.testQuery_08(TestCasesSprint_01.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at com.klarna.hiverunner.StandaloneHiveRunner.evaluateStatement(StandaloneHiveRunner.java:100)
	at com.klarna.hiverunner.StandaloneHiveRunner.access$000(StandaloneHiveRunner.java:53)
	at com.klarna.hiverunner.StandaloneHiveRunner$1$1.evaluate(StandaloneHiveRunner.java:79)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

15/05/25 20:39:41 ERROR ql.Driver: FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

org.apache.hadoop.hive.ql.parse.ParseException: line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:193)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:977)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)
	at com.klarna.hiverunner.HiveServerContainer.executeQuery(HiveServerContainer.java:90)
	at com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:76)
	at com.bdicm.app.TestCasesSprint_01.getActual(TestCasesSprint_01.java:125)
	at com.bdicm.app.TestCasesSprint_01.testQuery_08(TestCasesSprint_01.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at com.klarna.hiverunner.StandaloneHiveRunner.evaluateStatement(StandaloneHiveRunner.java:100)
	at com.klarna.hiverunner.StandaloneHiveRunner.access$000(StandaloneHiveRunner.java:53)
	at com.klarna.hiverunner.StandaloneHiveRunner$1$1.evaluate(StandaloneHiveRunner.java:79)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181282 end=1432597181312 duration=30>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181312 end=1432597181312 duration=0>
15/05/25 20:39:41 ERROR hiverunner.StandaloneHiveRunner: Failed to executeQuery Hive query SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname: Query returned non-zero code: 40000, cause: FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

java.lang.IllegalStateException: Failed to executeQuery Hive query SELECT c.cat_name, cli.cli_id, cli.cli_lastname, count(sale_id) FROM clients cli, sales s, products p, categories c WHERE cli.cli_id=s.sale_cli_id and s.sale_prod_id=p.prd_id and p.prd_category_id=c.cat_id and c.cat_id in (1) GROUP BY c.cat_name, cli.cli_id, cli.cli_lastname: Query returned non-zero code: 40000, cause: FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source

	at com.klarna.hiverunner.HiveServerContainer.executeQuery(HiveServerContainer.java:93)
	at com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:76)
	at com.bdicm.app.TestCasesSprint_01.getActual(TestCasesSprint_01.java:125)
	at com.bdicm.app.TestCasesSprint_01.testQuery_08(TestCasesSprint_01.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at com.klarna.hiverunner.StandaloneHiveRunner.evaluateStatement(StandaloneHiveRunner.java:100)
	at com.klarna.hiverunner.StandaloneHiveRunner.access$000(StandaloneHiveRunner.java:53)
	at com.klarna.hiverunner.StandaloneHiveRunner$1$1.evaluate(StandaloneHiveRunner.java:79)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: HiveServerException(message:Query returned non-zero code: 40000, cause: FAILED: ParseException line 1:80 cannot recognize input near 'cli' ',' 'sales' in table source
, errorCode:40000, SQLState:42000)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:219)
	at com.klarna.hiverunner.HiveServerContainer.executeQuery(HiveServerContainer.java:90)
	... 31 more
15/05/25 20:39:41 INFO service.HiveServer: Running the query: USE default
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: USE default
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181320 end=1432597181320 duration=0>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181321 end=1432597181321 duration=0>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181319 end=1432597181322 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: USE default
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181319 end=1432597181322 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181322 end=1432597181328 duration=6>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181322 end=1432597181328 duration=6>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181322 end=1432597181328 duration=6>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181328 end=1432597181329 duration=1>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181319 end=1432597181329 duration=10>
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Shutting down the object store...
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Metastore shutdown complete.
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15/05/25 20:39:41 INFO hiverunner.HiveServerContainer: Tore down HiveServer instance
]]></system-err>
  </testcase>
  <testcase name="testQuery_09" classname="com.bdicm.app.TestCasesSprint_01" time="2.34">
    <failure message="expected:&lt;[  	256.98, (Rx)  	261.55, 24	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.808, Amlodipine  	24.094, Amoxicillin  	238.944, Amphetamine  	56.779, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]&gt; but was:&lt;[  	256.97999999999996, (Rx)  	261.55, 24  	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.80799999999999, Amlodipine  	24.094, Amoxicillin  	238.94400000000002, Amphetamine  	56.778999999999996, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<[  	256.98, (Rx)  	261.55, 24	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.808, Amlodipine  	24.094, Amoxicillin  	238.944, Amphetamine  	56.779, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]> but was:<[  	256.97999999999996, (Rx)  	261.55, 24  	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.80799999999999, Amlodipine  	24.094, Amoxicillin  	238.94400000000002, Amphetamine  	56.778999999999996, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at com.bdicm.app.TestCasesSprint_01.testQuery_09(TestCasesSprint_01.java:199)
]]></failure>
    <system-err><![CDATA[15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/05/25 20:39:41 INFO metastore.ObjectStore: ObjectStore, initialize called
15/05/25 20:39:41 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
15/05/25 20:39:41 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15/05/25 20:39:41 INFO metastore.ObjectStore: Initialized ObjectStore
15/05/25 20:39:41 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.12.0
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Shutting down the object store...
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Metastore shutdown complete.
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15/05/25 20:39:41 INFO service.HiveServer: Putting temp output to file /tmp/junit2102258791818644947/localscratchdir/4482fc92-7f2c-4922-8299-f23e4532b1095764483702498929173.pipeout
15/05/25 20:39:41 INFO service.HiveServer: Running the query: SHOW TABLES
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: SHOW TABLES
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181650 end=1432597181651 duration=1>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181652 end=1432597181653 duration=1>
15/05/25 20:39:41 INFO exec.ListSinkOperator: Initializing Self 242 OP
15/05/25 20:39:41 INFO exec.ListSinkOperator: Operator 242 OP initialized
15/05/25 20:39:41 INFO exec.ListSinkOperator: Initialization Done 242 OP
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181650 end=1432597181656 duration=6>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: SHOW TABLES
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181649 end=1432597181657 duration=8>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/05/25 20:39:41 INFO metastore.ObjectStore: ObjectStore, initialize called
15/05/25 20:39:41 INFO metastore.ObjectStore: Initialized ObjectStore
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_tables: db=default pat=.*
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
15/05/25 20:39:41 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
15/05/25 20:39:41 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181657 end=1432597181723 duration=66>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181657 end=1432597181723 duration=66>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181656 end=1432597181724 duration=68>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181724 end=1432597181725 duration=1>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181649 end=1432597181725 duration=76>
15/05/25 20:39:41 INFO builder.HiveShellTearable: Executing script: create schema ${hiveconf:my.schema}
15/05/25 20:39:41 INFO service.HiveServer: Running the query: create schema ${hiveconf:my.schema}
15/05/25 20:39:41 INFO exec.ListSinkOperator: 242 finished. closing... 
15/05/25 20:39:41 INFO exec.ListSinkOperator: 242 forwarded 0 rows
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: create schema bar
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181728 end=1432597181729 duration=1>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181729 end=1432597181730 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181728 end=1432597181731 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: create schema bar
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181727 end=1432597181732 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_database: Database(name:bar, description:null, locationUri:null, parameters:null)
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:bar, description:null, locationUri:null, parameters:null)	
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: get_database: bar
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bar	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181732 end=1432597181745 duration=13>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181732 end=1432597181745 duration=13>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181731 end=1432597181746 duration=15>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181747 end=1432597181747 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181727 end=1432597181747 duration=20>
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/transactions/transactions_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/cards/cards_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/categories/categories_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/locations/locations_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/clients/clients_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/sales/sales_table.csv
15/05/25 20:39:41 INFO builder.HiveShellTearable: Created hive resource /tmp/junit2102258791818644947/hadooptmp/products/products_table.csv
15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/clients/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/clients/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181771 end=1432597181776 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table clients position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181777 end=1432597181779 duration=2>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181771 end=1432597181779 duration=8>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE clients( 
    cli_id INT, 
    cli_loc_id INT, 
    cli_firstname STRING, 
    cli_lastname STRING, 
    cli_phone STRING, 
    cli_cellphone STRING, 
    cli_cpf STRING, 
    cli_gender STRING, 
    cli_email STRING, 
    cli_password STRING, 
    cli_birthdate DATE, 
    cli_rg STRING, 
    cli_income STRING, 
    cli_token STRING 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/clients/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181771 end=1432597181782 duration=11>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table clients
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:clients, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cli_id, type:int, comment:null), FieldSchema(name:cli_loc_id, type:int, comment:null), FieldSchema(name:cli_firstname, type:string, comment:null), FieldSchema(name:cli_lastname, type:string, comment:null), FieldSchema(name:cli_phone, type:string, comment:null), FieldSchema(name:cli_cellphone, type:string, comment:null), FieldSchema(name:cli_cpf, type:string, comment:null), FieldSchema(name:cli_gender, type:string, comment:null), FieldSchema(name:cli_email, type:string, comment:null), FieldSchema(name:cli_password, type:string, comment:null), FieldSchema(name:cli_birthdate, type:date, comment:null), FieldSchema(name:cli_rg, type:string, comment:null), FieldSchema(name:cli_income, type:string, comment:null), FieldSchema(name:cli_token, type:string, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/clients, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:clients, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cli_id, type:int, comment:null), FieldSchema(name:cli_loc_id, type:int, comment:null), FieldSchema(name:cli_firstname, type:string, comment:null), FieldSchema(name:cli_lastname, type:string, comment:null), FieldSchema(name:cli_phone, type:string, comment:null), FieldSchema(name:cli_cellphone, type:string, comment:null), FieldSchema(name:cli_cpf, type:string, comment:null), FieldSchema(name:cli_gender, type:string, comment:null), FieldSchema(name:cli_email, type:string, comment:null), FieldSchema(name:cli_password, type:string, comment:null), FieldSchema(name:cli_birthdate, type:date, comment:null), FieldSchema(name:cli_rg, type:string, comment:null), FieldSchema(name:cli_income, type:string, comment:null), FieldSchema(name:cli_token, type:string, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/clients, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181782 end=1432597181854 duration=72>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181782 end=1432597181854 duration=72>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181780 end=1432597181854 duration=74>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181855 end=1432597181856 duration=1>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181770 end=1432597181856 duration=86>
15/05/25 20:39:41 INFO service.HiveServer: Running the query: 

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/transactions/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/transactions/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181862 end=1432597181869 duration=7>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table transactions position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181869 end=1432597181870 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181861 end=1432597181871 duration=10>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE transactions( 
    trans_id INT,     
    trans_cli_id INT, 
    trans_card_id INT, 
    trans_loc_id INT, 
    trans_date TIMESTAMP, 
    trans_value DOUBLE, 
    trans_lat DOUBLE, 
    trans_lon DOUBLE 
    ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/transactions/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181861 end=1432597181873 duration=12>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table transactions
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:transactions, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:trans_id, type:int, comment:null), FieldSchema(name:trans_cli_id, type:int, comment:null), FieldSchema(name:trans_card_id, type:int, comment:null), FieldSchema(name:trans_loc_id, type:int, comment:null), FieldSchema(name:trans_date, type:timestamp, comment:null), FieldSchema(name:trans_value, type:double, comment:null), FieldSchema(name:trans_lat, type:double, comment:null), FieldSchema(name:trans_lon, type:double, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/transactions, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:transactions, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:trans_id, type:int, comment:null), FieldSchema(name:trans_cli_id, type:int, comment:null), FieldSchema(name:trans_card_id, type:int, comment:null), FieldSchema(name:trans_loc_id, type:int, comment:null), FieldSchema(name:trans_date, type:timestamp, comment:null), FieldSchema(name:trans_value, type:double, comment:null), FieldSchema(name:trans_lat, type:double, comment:null), FieldSchema(name:trans_lon, type:double, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/transactions, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181873 end=1432597181919 duration=46>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181873 end=1432597181919 duration=46>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181872 end=1432597181919 duration=47>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181919 end=1432597181919 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181861 end=1432597181919 duration=58>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/categories/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/categories/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181924 end=1432597181926 duration=2>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table categories position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181926 end=1432597181926 duration=0>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181924 end=1432597181927 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE categories( 
    cat_id INT,
    cat_name  STRING, 
    cat_description STRING 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/categories/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181924 end=1432597181927 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table categories
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:categories, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cat_id, type:int, comment:null), FieldSchema(name:cat_name, type:string, comment:null), FieldSchema(name:cat_description, type:string, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/categories, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:categories, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:cat_id, type:int, comment:null), FieldSchema(name:cat_name, type:string, comment:null), FieldSchema(name:cat_description, type:string, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/categories, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181927 end=1432597181962 duration=35>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181927 end=1432597181962 duration=35>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181927 end=1432597181962 duration=35>
OK
15/05/25 20:39:41 INFO ql.Driver: OK
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597181962 end=1432597181962 duration=0>
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181924 end=1432597181962 duration=38>
15/05/25 20:39:41 INFO service.HiveServer: Running the query:  

15/05/25 20:39:41 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/cards/'
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:41 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/cards/'
15/05/25 20:39:41 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=parse start=1432597181963 end=1432597181966 duration=3>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:41 INFO parse.SemanticAnalyzer: Creating table ccards position=22
15/05/25 20:39:41 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597181966 end=1432597181967 duration=1>
15/05/25 20:39:41 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=compile start=1432597181963 end=1432597181968 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:41 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE ccards( 
    card_id INT, 
    card_band   STRING, 
    card_number STRING, 
    cardv_month INT, 
    cardv_year INT, 
    card_name STRING, 
    card_lastname STRING, 
    ccard_cvv INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/cards/'
15/05/25 20:39:41 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597181963 end=1432597181968 duration=5>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:41 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:41 INFO exec.DDLTask: Default to LazySimpleSerDe for table ccards
15/05/25 20:39:41 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:ccards, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:card_id, type:int, comment:null), FieldSchema(name:card_band, type:string, comment:null), FieldSchema(name:card_number, type:string, comment:null), FieldSchema(name:cardv_month, type:int, comment:null), FieldSchema(name:cardv_year, type:int, comment:null), FieldSchema(name:card_name, type:string, comment:null), FieldSchema(name:card_lastname, type:string, comment:null), FieldSchema(name:ccard_cvv, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/cards, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:41 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:ccards, dbName:default, owner:root, createTime:1432597181, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:card_id, type:int, comment:null), FieldSchema(name:card_band, type:string, comment:null), FieldSchema(name:card_number, type:string, comment:null), FieldSchema(name:cardv_month, type:int, comment:null), FieldSchema(name:cardv_year, type:int, comment:null), FieldSchema(name:card_name, type:string, comment:null), FieldSchema(name:card_lastname, type:string, comment:null), FieldSchema(name:ccard_cvv, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/cards, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597181968 end=1432597182028 duration=60>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=runTasks start=1432597181968 end=1432597182028 duration=60>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597181968 end=1432597182028 duration=60>
OK
15/05/25 20:39:42 INFO ql.Driver: OK
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597182028 end=1432597182028 duration=0>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597181963 end=1432597182028 duration=65>
15/05/25 20:39:42 INFO service.HiveServer: Running the query:  

15/05/25 20:39:42 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/locations/'
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:42 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/locations/'
15/05/25 20:39:42 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=parse start=1432597182029 end=1432597182030 duration=1>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Creating table locations position=22
15/05/25 20:39:42 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597182030 end=1432597182031 duration=1>
15/05/25 20:39:42 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=compile start=1432597182029 end=1432597182031 duration=2>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:42 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE locations( 
    loc_id INT, 
    loc_address STRING, 
    loc_city STRING, 
    loc_zipcode STRING, 
    loc_region STRING, 
    loc_country STRING, 
    loc_lat DOUBLE, 
    loc_lon DOUBLE 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/locations/'
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597182029 end=1432597182032 duration=3>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:42 INFO exec.DDLTask: Default to LazySimpleSerDe for table locations
15/05/25 20:39:42 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:locations, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:loc_id, type:int, comment:null), FieldSchema(name:loc_address, type:string, comment:null), FieldSchema(name:loc_city, type:string, comment:null), FieldSchema(name:loc_zipcode, type:string, comment:null), FieldSchema(name:loc_region, type:string, comment:null), FieldSchema(name:loc_country, type:string, comment:null), FieldSchema(name:loc_lat, type:double, comment:null), FieldSchema(name:loc_lon, type:double, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/locations, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:42 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:locations, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:loc_id, type:int, comment:null), FieldSchema(name:loc_address, type:string, comment:null), FieldSchema(name:loc_city, type:string, comment:null), FieldSchema(name:loc_zipcode, type:string, comment:null), FieldSchema(name:loc_region, type:string, comment:null), FieldSchema(name:loc_country, type:string, comment:null), FieldSchema(name:loc_lat, type:double, comment:null), FieldSchema(name:loc_lon, type:double, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/locations, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597182032 end=1432597182051 duration=19>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=runTasks start=1432597182032 end=1432597182052 duration=20>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597182031 end=1432597182052 duration=21>
OK
15/05/25 20:39:42 INFO ql.Driver: OK
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597182053 end=1432597182053 duration=0>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597182028 end=1432597182053 duration=25>
15/05/25 20:39:42 INFO service.HiveServer: Running the query: 

15/05/25 20:39:42 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/products/'
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:42 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/products/'
15/05/25 20:39:42 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=parse start=1432597182055 end=1432597182057 duration=2>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Creating table products position=22
15/05/25 20:39:42 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597182057 end=1432597182058 duration=1>
15/05/25 20:39:42 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=compile start=1432597182054 end=1432597182058 duration=4>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:42 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE products( 
    prd_id INT, 
    prd_name  STRING, 
    prd_value  DOUBLE, 
    prd_descr  STRING, 
    prd_category_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/products/'
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597182054 end=1432597182059 duration=5>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:42 INFO exec.DDLTask: Default to LazySimpleSerDe for table products
15/05/25 20:39:42 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:products, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:prd_id, type:int, comment:null), FieldSchema(name:prd_name, type:string, comment:null), FieldSchema(name:prd_value, type:double, comment:null), FieldSchema(name:prd_descr, type:string, comment:null), FieldSchema(name:prd_category_id, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/products, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:42 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:products, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:prd_id, type:int, comment:null), FieldSchema(name:prd_name, type:string, comment:null), FieldSchema(name:prd_value, type:double, comment:null), FieldSchema(name:prd_descr, type:string, comment:null), FieldSchema(name:prd_category_id, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/products, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597182059 end=1432597182080 duration=21>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=runTasks start=1432597182059 end=1432597182080 duration=21>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597182058 end=1432597182080 duration=22>
OK
15/05/25 20:39:42 INFO ql.Driver: OK
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597182081 end=1432597182081 duration=0>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597182054 end=1432597182081 duration=27>
15/05/25 20:39:42 INFO service.HiveServer: Running the query:  

15/05/25 20:39:42 INFO service.HiveServer: Running the query: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '${hiveconf:MY.HDFS.DIR}/sales/'
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:42 INFO parse.ParseDriver: Parsing command: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/sales/'
15/05/25 20:39:42 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=parse start=1432597182083 end=1432597182085 duration=2>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Creating table sales position=22
15/05/25 20:39:42 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597182085 end=1432597182086 duration=1>
15/05/25 20:39:42 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=compile start=1432597182083 end=1432597182087 duration=4>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:42 INFO ql.Driver: Starting command: CREATE EXTERNAL TABLE sales( 
    sale_id INT, 
    sale_cli_id  STRING, 
    sale_value  DOUBLE, 
    sale_vol     INT, 
    sale_date  TIMESTAMP, 
    sale_prod_id INT 
     ) 
    ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '|' 
    LOCATION '/tmp/junit2102258791818644947/hadooptmp/sales/'
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597182083 end=1432597182088 duration=5>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:42 INFO exec.DDLTask: Default to LazySimpleSerDe for table sales
15/05/25 20:39:42 INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:sales, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:sale_id, type:int, comment:null), FieldSchema(name:sale_cli_id, type:string, comment:null), FieldSchema(name:sale_value, type:double, comment:null), FieldSchema(name:sale_vol, type:int, comment:null), FieldSchema(name:sale_date, type:timestamp, comment:null), FieldSchema(name:sale_prod_id, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/sales, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))
15/05/25 20:39:42 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:sales, dbName:default, owner:root, createTime:1432597182, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:sale_id, type:int, comment:null), FieldSchema(name:sale_cli_id, type:string, comment:null), FieldSchema(name:sale_value, type:double, comment:null), FieldSchema(name:sale_vol, type:int, comment:null), FieldSchema(name:sale_date, type:timestamp, comment:null), FieldSchema(name:sale_prod_id, type:int, comment:null)], location:/tmp/junit2102258791818644947/hadooptmp/sales, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))	
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597182088 end=1432597182104 duration=16>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=runTasks start=1432597182088 end=1432597182104 duration=16>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597182087 end=1432597182104 duration=17>
OK
15/05/25 20:39:42 INFO ql.Driver: OK
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597182105 end=1432597182105 duration=0>
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597182083 end=1432597182105 duration=22>
15/05/25 20:39:42 INFO service.HiveServer: Running the query:  

15/05/25 20:39:42 INFO service.HiveServer: Running the query: SELECT products.prd_name , SUM(sales.sale_value) as sales_total FROM products JOIN sales ON (sales.sale_prod_id = products.prd_id AND sales.sale_date < "2015-04-04" AND sales.sale_date > "2014-03-04") GROUP BY products.prd_name ORDER BY sales_total DESC LIMIT 250
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:42 INFO parse.ParseDriver: Parsing command: SELECT products.prd_name , SUM(sales.sale_value) as sales_total FROM products JOIN sales ON (sales.sale_prod_id = products.prd_id AND sales.sale_date < "2015-04-04" AND sales.sale_date > "2014-03-04") GROUP BY products.prd_name ORDER BY sales_total DESC LIMIT 250
15/05/25 20:39:42 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=parse start=1432597182107 end=1432597182112 duration=5>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Get metadata for source tables
15/05/25 20:39:42 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=sales
15/05/25 20:39:42 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=sales	
15/05/25 20:39:42 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=products
15/05/25 20:39:42 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=products	
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Get metadata for subqueries
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Get metadata for destination tables
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for FS(258)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for LIM(257)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for EX(256)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for RS(255)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for SEL(254)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for GBY(253)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for RS(252)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for GBY(251)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for SEL(250)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for JOIN(249)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for RS(248)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for FIL(246)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Pushdown Predicates of FIL For Alias : sales
15/05/25 20:39:42 INFO ppd.OpProcFactory: 	(sale_date > '2014-03-04')
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for FIL(245)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Pushdown Predicates of FIL For Alias : sales
15/05/25 20:39:42 INFO ppd.OpProcFactory: 	(sale_date < '2015-04-04')
15/05/25 20:39:42 INFO ppd.OpProcFactory: Pushdown Predicates of FIL For Alias : sales
15/05/25 20:39:42 INFO ppd.OpProcFactory: 	(sale_date > '2014-03-04')
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for TS(243)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Pushdown Predicates of TS For Alias : sales
15/05/25 20:39:42 INFO ppd.OpProcFactory: 	(sale_date < '2015-04-04')
15/05/25 20:39:42 INFO ppd.OpProcFactory: 	(sale_date > '2014-03-04')
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for RS(247)
15/05/25 20:39:42 INFO ppd.OpProcFactory: Processing for TS(244)
15/05/25 20:39:42 INFO optimizer.ColumnPrunerProcFactory: RS 248 oldColExprMap: {VALUE._col5=Column[sale_prod_id], VALUE._col4=Column[sale_date], VALUE._col3=Column[sale_vol], VALUE._col2=Column[sale_value], VALUE._col1=Column[sale_cli_id], VALUE._col0=Column[sale_id], VALUE._col7=Column[INPUT__FILE__NAME], VALUE._col6=Column[BLOCK__OFFSET__INSIDE__FILE]}
15/05/25 20:39:42 INFO optimizer.ColumnPrunerProcFactory: RS 248 newColExprMap: {VALUE._col5=Column[sale_prod_id], VALUE._col2=Column[sale_value]}
15/05/25 20:39:42 INFO optimizer.ColumnPrunerProcFactory: RS 247 oldColExprMap: {VALUE._col5=Column[BLOCK__OFFSET__INSIDE__FILE], VALUE._col4=Column[prd_category_id], VALUE._col3=Column[prd_descr], VALUE._col2=Column[prd_value], VALUE._col1=Column[prd_name], VALUE._col0=Column[prd_id], VALUE._col6=Column[INPUT__FILE__NAME]}
15/05/25 20:39:42 INFO optimizer.ColumnPrunerProcFactory: RS 247 newColExprMap: {VALUE._col1=Column[prd_name], VALUE._col0=Column[prd_id]}
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO parse.SemanticAnalyzer: Completed plan generation
15/05/25 20:39:42 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597182113 end=1432597182298 duration=185>
15/05/25 20:39:42 INFO exec.ListSinkOperator: Initializing Self 264 OP
15/05/25 20:39:42 INFO exec.ListSinkOperator: Operator 264 OP initialized
15/05/25 20:39:42 INFO exec.ListSinkOperator: Initialization Done 264 OP
15/05/25 20:39:42 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:prd_name, type:string, comment:null), FieldSchema(name:sales_total, type:double, comment:null)], properties:null)
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=compile start=1432597182107 end=1432597182299 duration=192>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:42 INFO ql.Driver: Starting command: SELECT products.prd_name , SUM(sales.sale_value) as sales_total FROM products JOIN sales ON (sales.sale_prod_id = products.prd_id AND sales.sale_date < "2015-04-04" AND sales.sale_date > "2014-03-04") GROUP BY products.prd_name ORDER BY sales_total DESC LIMIT 250
Total MapReduce jobs = 3
15/05/25 20:39:42 INFO ql.Driver: Total MapReduce jobs = 3
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597182107 end=1432597182300 duration=193>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=task.MAPRED.Stage-1>
Launching Job 1 out of 3
15/05/25 20:39:42 INFO ql.Driver: Launching Job 1 out of 3
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=getInputSummary>
15/05/25 20:39:42 INFO exec.Utilities: Cache Content Summary for file:/tmp/junit2102258791818644947/hadooptmp/products length: 20038 file count: 1 directory count: 1
15/05/25 20:39:42 INFO exec.Utilities: Cache Content Summary for file:/tmp/junit2102258791818644947/hadooptmp/sales length: 83540 file count: 1 directory count: 1
15/05/25 20:39:42 INFO exec.Utilities: </PERFLOG method=getInputSummary start=1432597182301 end=1432597182310 duration=9>
15/05/25 20:39:42 INFO exec.Utilities: BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=103578
Number of reduce tasks not specified. Estimated from input data size: 1
15/05/25 20:39:42 INFO exec.Task: Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
15/05/25 20:39:42 INFO exec.Task: In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
15/05/25 20:39:42 INFO exec.Task:   set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
15/05/25 20:39:42 INFO exec.Task: In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
15/05/25 20:39:42 INFO exec.Task:   set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
15/05/25 20:39:42 INFO exec.Task: In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
15/05/25 20:39:42 INFO exec.Task:   set mapred.reduce.tasks=<number>
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO mr.ExecDriver: Using org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
15/05/25 20:39:42 INFO exec.Utilities: Processing alias sales
15/05/25 20:39:42 INFO exec.Utilities: Adding input file file:/tmp/junit2102258791818644947/hadooptmp/sales
15/05/25 20:39:42 INFO exec.Utilities: Content Summary file:/tmp/junit2102258791818644947/hadooptmp/saleslength: 83540 num files: 1 num directories: 1
15/05/25 20:39:42 INFO exec.Utilities: Processing alias products
15/05/25 20:39:42 INFO exec.Utilities: Adding input file file:/tmp/junit2102258791818644947/hadooptmp/products
15/05/25 20:39:42 INFO exec.Utilities: Content Summary file:/tmp/junit2102258791818644947/hadooptmp/productslength: 20038 num files: 1 num directories: 1
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:42 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597182317 end=1432597182443 duration=126>
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:42 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597182444 end=1432597182564 duration=120>
15/05/25 20:39:42 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
15/05/25 20:39:42 INFO io.CombineHiveInputFormat: <PERFLOG method=getSplits>
15/05/25 20:39:42 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for file:/tmp/junit2102258791818644947/hadooptmp/sales; using filter path file:/tmp/junit2102258791818644947/hadooptmp/sales
15/05/25 20:39:42 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for file:/tmp/junit2102258791818644947/hadooptmp/products; using filter path file:/tmp/junit2102258791818644947/hadooptmp/products
15/05/25 20:39:42 INFO mapred.FileInputFormat: Total input paths to process : 2
15/05/25 20:39:42 INFO io.CombineHiveInputFormat: number of splits 2
15/05/25 20:39:42 INFO io.CombineHiveInputFormat: </PERFLOG method=getSplits start=1432597182633 end=1432597182637 duration=4>
15/05/25 20:39:42 INFO filecache.TrackerDistributedCacheManager: Creating map.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-4054852360808529248_-265388056_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417-work--5731299468188280258 with rwxr-xr-x
15/05/25 20:39:42 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/map.xml#map.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-4054852360808529248_-265388056_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/map.xml
15/05/25 20:39:42 INFO filecache.TrackerDistributedCacheManager: Creating reduce.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-6447989692289301573_-536681072_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417-work--7912398597086642931 with rwxr-xr-x
15/05/25 20:39:42 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/reduce.xml#reduce.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-6447989692289301573_-536681072_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/reduce.xml
15/05/25 20:39:42 WARN mapred.LocalJobRunner: LocalJobRunner does not support symlinking into current working dir.
15/05/25 20:39:42 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-4054852360808529248_-265388056_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/map.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml
15/05/25 20:39:42 WARN fs.FileUtil: Command 'ln -s /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-4054852360808529248_-265388056_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/map.xml /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml' failed 1 with: ln: failed to create symbolic link /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml: No such file or directory

15/05/25 20:39:42 WARN mapred.TaskRunner: Failed to create symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-4054852360808529248_-265388056_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/map.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml
15/05/25 20:39:42 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-6447989692289301573_-536681072_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/reduce.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml
15/05/25 20:39:42 WARN fs.FileUtil: Command 'ln -s /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-6447989692289301573_-536681072_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/reduce.xml /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml' failed 1 with: ln: failed to create symbolic link /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml: No such file or directory

15/05/25 20:39:42 WARN mapred.TaskRunner: Failed to create symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-6447989692289301573_-536681072_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10005/845efe67-f6ce-449a-8a3a-7e20aa7ce417/reduce.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml
Starting Job = job_local1145071447_0015, Tracking URL = http://localhost:8080/
15/05/25 20:39:42 INFO exec.Task: Starting Job = job_local1145071447_0015, Tracking URL = http://localhost:8080/
Kill Command = NO_BIN! job  -kill job_local1145071447_0015
15/05/25 20:39:42 INFO exec.Task: Kill Command = NO_BIN! job  -kill job_local1145071447_0015
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Waiting for map tasks
Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
15/05/25 20:39:42 INFO exec.Task: Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
2015-05-25 20:39:42,689 Stage-1 map = 0%,  reduce = 0%
15/05/25 20:39:42 INFO exec.Task: 2015-05-25 20:39:42,689 Stage-1 map = 0%,  reduce = 0%
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1145071447_0015_m_000000_0
15/05/25 20:39:42 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@1e26253a
15/05/25 20:39:42 INFO mapred.MapTask: Processing split: Paths:/tmp/junit2102258791818644947/hadooptmp/sales/sales_table.csv:0+83540InputFormatClass: org.apache.hadoop.mapred.TextInputFormat

15/05/25 20:39:42 INFO io.HiveContextAwareRecordReader: Processing file file:/tmp/junit2102258791818644947/hadooptmp/sales/sales_table.csv
15/05/25 20:39:42 INFO mapred.MapTask: numReduceTasks: 1
15/05/25 20:39:42 INFO mapred.MapTask: io.sort.mb = 100
15/05/25 20:39:42 INFO mapred.MapTask: data buffer = 79691776/99614720
15/05/25 20:39:42 INFO mapred.MapTask: record buffer = 262144/327680
15/05/25 20:39:42 INFO mr.ExecMapper: maximum memory = 922746880
15/05/25 20:39:42 INFO mr.ExecMapper: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO mr.ExecMapper: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO exec.MapOperator: Adding alias sales to work list for file file:/tmp/junit2102258791818644947/hadooptmp/sales
15/05/25 20:39:42 INFO exec.MapOperator: dump TS struct<sale_id:int,sale_cli_id:string,sale_value:double,sale_vol:int,sale_date:timestamp,sale_prod_id:int>
15/05/25 20:39:42 INFO exec.MapOperator: Adding alias products to work list for file file:/tmp/junit2102258791818644947/hadooptmp/products
15/05/25 20:39:42 INFO mr.ExecMapper: 
<MAP>Id =279
  <Children>
    <TS>Id =243
      <Children>
        <FIL>Id =259
          <Children>
            <RS>Id =248
              <Parent>Id = 259 null<\Parent>
            <\RS>
          <\Children>
          <Parent>Id = 243 null<\Parent>
        <\FIL>
      <\Children>
      <Parent>Id = 279 null<\Parent>
    <\TS>
  <\Children>
<\MAP>
15/05/25 20:39:42 INFO exec.MapOperator: Initializing Self 279 MAP
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing Self 243 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Operator 243 TS initialized
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing children of 243 TS
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing child 259 FIL
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing Self 259 FIL
15/05/25 20:39:42 INFO exec.FilterOperator: Operator 259 FIL initialized
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing children of 259 FIL
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing child 248 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing Self 248 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Using tag = 1
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Operator 248 RS initialized
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initialization Done 248 RS
15/05/25 20:39:42 INFO exec.FilterOperator: Initialization Done 259 FIL
15/05/25 20:39:42 INFO exec.TableScanOperator: Initialization Done 243 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing Self 244 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Operator 244 TS initialized
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing children of 244 TS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing child 247 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing Self 247 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Using tag = 0
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Operator 247 RS initialized
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initialization Done 247 RS
15/05/25 20:39:42 INFO exec.TableScanOperator: Initialization Done 244 TS
15/05/25 20:39:42 INFO exec.MapOperator: Initialization Done 279 MAP
15/05/25 20:39:42 INFO exec.MapOperator: Processing alias sales for file file:/tmp/junit2102258791818644947/hadooptmp/sales
15/05/25 20:39:42 INFO exec.MapOperator: 279 forwarding 1 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarding 1 rows
15/05/25 20:39:42 INFO exec.FilterOperator: 259 forwarding 1 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 1 rows: used memory = 220801192
15/05/25 20:39:42 INFO exec.MapOperator: 279 forwarding 10 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarding 10 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 10 rows: used memory = 220801192
15/05/25 20:39:42 INFO exec.FilterOperator: 259 forwarding 10 rows
15/05/25 20:39:42 INFO exec.MapOperator: 279 forwarding 100 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarding 100 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 100 rows: used memory = 220801192
15/05/25 20:39:42 INFO exec.FilterOperator: 259 forwarding 100 rows
15/05/25 20:39:42 INFO exec.MapOperator: 279 forwarding 1000 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarding 1000 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 1000 rows: used memory = 223779256
15/05/25 20:39:42 INFO exec.MapOperator: 279 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 forwarded 0 rows
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 247 finished. closing... 
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 247 forwarded 0 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 Close done
15/05/25 20:39:42 INFO exec.MapOperator: 279 forwarded 2000 rows
15/05/25 20:39:42 INFO exec.MapOperator: DESERIALIZE_ERRORS:0
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarded 2000 rows
15/05/25 20:39:42 INFO exec.FilterOperator: 259 finished. closing... 
15/05/25 20:39:42 INFO exec.FilterOperator: 259 forwarded 710 rows
15/05/25 20:39:42 INFO exec.FilterOperator: PASSED:710
15/05/25 20:39:42 INFO exec.FilterOperator: FILTERED:1290
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 248 finished. closing... 
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 248 forwarded 0 rows
15/05/25 20:39:42 INFO exec.FilterOperator: 259 Close done
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 Close done
15/05/25 20:39:42 INFO exec.MapOperator: 279 Close done
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processed 2000 rows: used memory = 226757296
15/05/25 20:39:42 INFO mapred.MapTask: Starting flush of map output
15/05/25 20:39:42 INFO mapred.MapTask: Finished spill 0
15/05/25 20:39:42 INFO mapred.Task: Task:attempt_local1145071447_0015_m_000000_0 is done. And is in the process of commiting
15/05/25 20:39:42 INFO mapred.LocalJobRunner: file:/tmp/junit2102258791818644947/hadooptmp/sales/sales_table.csv:0+83540
15/05/25 20:39:42 INFO mapred.Task: Task 'attempt_local1145071447_0015_m_000000_0' done.
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1145071447_0015_m_000000_0
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1145071447_0015_m_000001_0
15/05/25 20:39:42 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@4ea77b1b
15/05/25 20:39:42 INFO mapred.MapTask: Processing split: Paths:/tmp/junit2102258791818644947/hadooptmp/products/products_table.csv:0+20038InputFormatClass: org.apache.hadoop.mapred.TextInputFormat

15/05/25 20:39:42 INFO io.HiveContextAwareRecordReader: Processing file file:/tmp/junit2102258791818644947/hadooptmp/products/products_table.csv
15/05/25 20:39:42 INFO mapred.MapTask: numReduceTasks: 1
15/05/25 20:39:42 INFO mapred.MapTask: io.sort.mb = 100
15/05/25 20:39:42 INFO mapred.MapTask: data buffer = 79691776/99614720
15/05/25 20:39:42 INFO mapred.MapTask: record buffer = 262144/327680
15/05/25 20:39:42 INFO mr.ExecMapper: maximum memory = 922746880
15/05/25 20:39:42 INFO mr.ExecMapper: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO mr.ExecMapper: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO exec.MapOperator: Adding alias sales to work list for file file:/tmp/junit2102258791818644947/hadooptmp/sales
15/05/25 20:39:42 INFO exec.MapOperator: Adding alias products to work list for file file:/tmp/junit2102258791818644947/hadooptmp/products
15/05/25 20:39:42 INFO exec.MapOperator: dump TS struct<prd_id:int,prd_name:string,prd_value:double,prd_descr:string,prd_category_id:int>
15/05/25 20:39:42 INFO mr.ExecMapper: 
<MAP>Id =280
  <Children>
    <TS>Id =244
      <Children>
        <RS>Id =247
          <Parent>Id = 244 null<\Parent>
        <\RS>
      <\Children>
      <Parent>Id = 280 null<\Parent>
    <\TS>
  <\Children>
<\MAP>
15/05/25 20:39:42 INFO exec.MapOperator: Initializing Self 280 MAP
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing Self 244 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Operator 244 TS initialized
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing children of 244 TS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing child 247 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing Self 247 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Using tag = 0
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Operator 247 RS initialized
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initialization Done 247 RS
15/05/25 20:39:42 INFO exec.TableScanOperator: Initialization Done 244 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing Self 243 TS
15/05/25 20:39:42 INFO exec.TableScanOperator: Operator 243 TS initialized
15/05/25 20:39:42 INFO exec.TableScanOperator: Initializing children of 243 TS
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing child 259 FIL
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing Self 259 FIL
15/05/25 20:39:42 INFO exec.FilterOperator: Operator 259 FIL initialized
15/05/25 20:39:42 INFO exec.FilterOperator: Initializing children of 259 FIL
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing child 248 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initializing Self 248 RS
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Using tag = 1
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Operator 248 RS initialized
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: Initialization Done 248 RS
15/05/25 20:39:42 INFO exec.FilterOperator: Initialization Done 259 FIL
15/05/25 20:39:42 INFO exec.TableScanOperator: Initialization Done 243 TS
15/05/25 20:39:42 INFO exec.MapOperator: Initialization Done 280 MAP
15/05/25 20:39:42 INFO exec.MapOperator: Processing alias products for file file:/tmp/junit2102258791818644947/hadooptmp/products
15/05/25 20:39:42 INFO exec.MapOperator: 280 forwarding 1 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 forwarding 1 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 1 rows: used memory = 334617624
15/05/25 20:39:42 INFO exec.MapOperator: 280 forwarding 10 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 forwarding 10 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 10 rows: used memory = 334617624
15/05/25 20:39:42 INFO exec.MapOperator: 280 forwarding 100 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 forwarding 100 rows
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processing 100 rows: used memory = 336131200
15/05/25 20:39:42 INFO exec.MapOperator: 280 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 forwarded 2000 rows
15/05/25 20:39:42 INFO exec.FilterOperator: 259 finished. closing... 
15/05/25 20:39:42 INFO exec.FilterOperator: 259 forwarded 710 rows
15/05/25 20:39:42 INFO exec.FilterOperator: PASSED:710
15/05/25 20:39:42 INFO exec.FilterOperator: FILTERED:1290
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 248 finished. closing... 
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 248 forwarded 0 rows
15/05/25 20:39:42 INFO exec.FilterOperator: 259 Close done
15/05/25 20:39:42 INFO exec.TableScanOperator: 243 Close done
15/05/25 20:39:42 INFO exec.MapOperator: 280 forwarded 296 rows
15/05/25 20:39:42 INFO exec.MapOperator: DESERIALIZE_ERRORS:0
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 finished. closing... 
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 forwarded 296 rows
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 247 finished. closing... 
15/05/25 20:39:42 INFO exec.ReduceSinkOperator: 247 forwarded 0 rows
15/05/25 20:39:42 INFO exec.TableScanOperator: 244 Close done
15/05/25 20:39:42 INFO exec.MapOperator: 280 Close done
15/05/25 20:39:42 INFO mr.ExecMapper: ExecMapper: processed 296 rows: used memory = 336131200
15/05/25 20:39:42 INFO mapred.MapTask: Starting flush of map output
15/05/25 20:39:42 INFO mapred.MapTask: Finished spill 0
15/05/25 20:39:42 INFO mapred.Task: Task:attempt_local1145071447_0015_m_000001_0 is done. And is in the process of commiting
15/05/25 20:39:42 INFO mapred.LocalJobRunner: file:/tmp/junit2102258791818644947/hadooptmp/products/products_table.csv:0+20038
15/05/25 20:39:42 INFO mapred.Task: Task 'attempt_local1145071447_0015_m_000001_0' done.
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1145071447_0015_m_000001_0
15/05/25 20:39:42 INFO mapred.LocalJobRunner: Map task executor complete.
15/05/25 20:39:42 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@1b2eb620
15/05/25 20:39:42 INFO mapred.LocalJobRunner: 
15/05/25 20:39:42 INFO mapred.Merger: Merging 2 sorted segments
15/05/25 20:39:42 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 26098 bytes
15/05/25 20:39:42 INFO mapred.LocalJobRunner: 
15/05/25 20:39:42 INFO ExecReducer: maximum memory = 922746880
15/05/25 20:39:42 INFO ExecReducer: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO ExecReducer: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:42 INFO ExecReducer: 
<JOIN>Id =249
  <Children>
    <SEL>Id =250
      <Children>
        <GBY>Id =251
          <Children>
            <FS>Id =260
              <Parent>Id = 251 null<\Parent>
            <\FS>
          <\Children>
          <Parent>Id = 250 null<\Parent>
        <\GBY>
      <\Children>
      <Parent>Id = 249 null<\Parent>
    <\SEL>
  <\Children>
<\JOIN>
15/05/25 20:39:42 INFO exec.JoinOperator: Initializing Self 249 JOIN
15/05/25 20:39:42 INFO exec.CommonJoinOperator: JOIN struct<_col1:string,_col9:double> totalsz = 2
15/05/25 20:39:42 INFO exec.JoinOperator: Operator 249 JOIN initialized
15/05/25 20:39:42 INFO exec.JoinOperator: Initializing children of 249 JOIN
15/05/25 20:39:42 INFO exec.SelectOperator: Initializing child 250 SEL
15/05/25 20:39:42 INFO exec.SelectOperator: Initializing Self 250 SEL
15/05/25 20:39:42 INFO exec.SelectOperator: SELECT struct<_col1:string,_col9:double>
15/05/25 20:39:42 INFO exec.SelectOperator: Operator 250 SEL initialized
15/05/25 20:39:42 INFO exec.SelectOperator: Initializing children of 250 SEL
15/05/25 20:39:42 INFO exec.GroupByOperator: Initializing child 251 GBY
15/05/25 20:39:42 INFO exec.GroupByOperator: Initializing Self 251 GBY
15/05/25 20:39:42 INFO exec.GroupByOperator: Operator 251 GBY initialized
15/05/25 20:39:42 INFO exec.GroupByOperator: Initializing children of 251 GBY
15/05/25 20:39:42 INFO exec.FileSinkOperator: Initializing child 260 FS
15/05/25 20:39:42 INFO exec.FileSinkOperator: Initializing Self 260 FS
15/05/25 20:39:42 INFO exec.FileSinkOperator: Operator 260 FS initialized
15/05/25 20:39:42 INFO exec.FileSinkOperator: Initialization Done 260 FS
15/05/25 20:39:42 INFO exec.GroupByOperator: Initialization Done 251 GBY
15/05/25 20:39:42 INFO exec.SelectOperator: Initialization Done 250 SEL
15/05/25 20:39:42 INFO exec.JoinOperator: Initialization Done 249 JOIN
15/05/25 20:39:42 INFO ExecReducer: ExecReducer: processing 1 rows: used memory = 339109408
15/05/25 20:39:42 INFO exec.JoinOperator: 249 forwarding 1 rows
15/05/25 20:39:42 INFO exec.SelectOperator: 250 forwarding 1 rows
15/05/25 20:39:42 INFO ExecReducer: ExecReducer: processing 10 rows: used memory = 339109408
15/05/25 20:39:42 INFO exec.JoinOperator: 249 forwarding 10 rows
15/05/25 20:39:42 INFO exec.SelectOperator: 250 forwarding 10 rows
15/05/25 20:39:42 INFO ExecReducer: ExecReducer: processing 100 rows: used memory = 339109408
15/05/25 20:39:42 INFO exec.JoinOperator: 249 forwarding 100 rows
15/05/25 20:39:42 INFO exec.SelectOperator: 250 forwarding 100 rows
15/05/25 20:39:42 INFO ExecReducer: ExecReducer: processing 1000 rows: used memory = 339109408
15/05/25 20:39:42 INFO ExecReducer: ExecReducer: processed 1006 rows: used memory = 339109408
15/05/25 20:39:42 INFO exec.JoinOperator: 249 finished. closing... 
15/05/25 20:39:42 INFO exec.JoinOperator: 249 forwarded 710 rows
15/05/25 20:39:42 INFO exec.JoinOperator: SKEWJOINFOLLOWUPJOBS:0
15/05/25 20:39:42 INFO exec.SelectOperator: 250 finished. closing... 
15/05/25 20:39:42 INFO exec.SelectOperator: 250 forwarded 710 rows
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 finished. closing... 
15/05/25 20:39:42 INFO exec.GroupByOperator: Begin Hash Table flush: size = 112
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 forwarding 1 rows
15/05/25 20:39:42 INFO exec.FileSinkOperator: Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10002/000000_0
15/05/25 20:39:42 INFO exec.FileSinkOperator: Writing to temp file: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_task_tmp.-mr-10002/_tmp.000000_0
15/05/25 20:39:42 INFO exec.FileSinkOperator: New Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10002/000000_0
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 forwarding 10 rows
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 forwarding 100 rows
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 forwarded 112 rows
15/05/25 20:39:42 INFO exec.FileSinkOperator: 260 finished. closing... 
15/05/25 20:39:42 INFO exec.FileSinkOperator: 260 forwarded 0 rows
15/05/25 20:39:42 INFO exec.GroupByOperator: 251 Close done
15/05/25 20:39:42 INFO exec.SelectOperator: 250 Close done
15/05/25 20:39:42 INFO exec.JoinOperator: 249 Close done
15/05/25 20:39:42 INFO mapred.Task: Task:attempt_local1145071447_0015_r_000000_0 is done. And is in the process of commiting
15/05/25 20:39:42 INFO mapred.LocalJobRunner: reduce > reduce
15/05/25 20:39:42 INFO mapred.Task: Task 'attempt_local1145071447_0015_r_000000_0' done.
2015-05-25 20:39:42,947 Stage-1 map = 0%,  reduce = 100%
15/05/25 20:39:42 INFO exec.Task: 2015-05-25 20:39:42,947 Stage-1 map = 0%,  reduce = 100%
Ended Job = job_local1145071447_0015
15/05/25 20:39:42 INFO exec.Task: Ended Job = job_local1145071447_0015
15/05/25 20:39:42 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10002 to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10002.intermediate
15/05/25 20:39:42 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10002.intermediate to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:42 INFO ql.Driver: </PERFLOG method=task.MAPRED.Stage-1 start=1432597182300 end=1432597182948 duration=648>
15/05/25 20:39:42 INFO ql.Driver: <PERFLOG method=task.MAPRED.Stage-2>
Launching Job 2 out of 3
15/05/25 20:39:42 INFO ql.Driver: Launching Job 2 out of 3
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=getInputSummary>
15/05/25 20:39:42 INFO exec.Utilities: Cache Content Summary for file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002 length: 4235 file count: 1 directory count: 1
15/05/25 20:39:42 INFO exec.Utilities: </PERFLOG method=getInputSummary start=1432597182949 end=1432597182950 duration=1>
15/05/25 20:39:42 INFO exec.Utilities: BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=4235
Number of reduce tasks not specified. Estimated from input data size: 1
15/05/25 20:39:42 INFO exec.Task: Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
15/05/25 20:39:42 INFO exec.Task: In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
15/05/25 20:39:42 INFO exec.Task:   set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
15/05/25 20:39:42 INFO exec.Task: In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
15/05/25 20:39:42 INFO exec.Task:   set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
15/05/25 20:39:42 INFO exec.Task: In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
15/05/25 20:39:42 INFO exec.Task:   set mapred.reduce.tasks=<number>
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO mr.ExecDriver: Using org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
15/05/25 20:39:42 INFO exec.Utilities: Processing alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:42 INFO exec.Utilities: Adding input file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:42 INFO exec.Utilities: Content Summary file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002length: 4235 num files: 1 num directories: 1
15/05/25 20:39:42 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:42 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597182953 end=1432597182985 duration=32>
15/05/25 20:39:42 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:43 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597182987 end=1432597183048 duration=61>
15/05/25 20:39:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: <PERFLOG method=getSplits>
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002; using filter path file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:43 INFO mapred.FileInputFormat: Total input paths to process : 1
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: number of splits 1
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: </PERFLOG method=getSplits start=1432597183122 end=1432597183124 duration=2>
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Creating map.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/5166996297887647769_-1031268647_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de-work--639986802818910928 with rwxr-xr-x
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/map.xml#map.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/5166996297887647769_-1031268647_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/map.xml
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Creating reduce.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/7494815348987629165_-2019091201_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de-work--7306391920514320691 with rwxr-xr-x
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/reduce.xml#reduce.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/7494815348987629165_-2019091201_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/reduce.xml
15/05/25 20:39:43 WARN mapred.LocalJobRunner: LocalJobRunner does not support symlinking into current working dir.
15/05/25 20:39:43 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/5166996297887647769_-1031268647_225589451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/map.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml
15/05/25 20:39:43 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/7494815348987629165_-2019091201_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10007/d596d8bf-44f8-4394-bd25-ce64f775d6de/reduce.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml
Starting Job = job_local243227878_0016, Tracking URL = http://localhost:8080/
15/05/25 20:39:43 INFO exec.Task: Starting Job = job_local243227878_0016, Tracking URL = http://localhost:8080/
Kill Command = NO_BIN! job  -kill job_local243227878_0016
15/05/25 20:39:43 INFO exec.Task: Kill Command = NO_BIN! job  -kill job_local243227878_0016
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Waiting for map tasks
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Starting task: attempt_local243227878_0016_m_000000_0
15/05/25 20:39:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6432817e
Hadoop job information for Stage-2: number of mappers: 0; number of reducers: 0
15/05/25 20:39:43 INFO exec.Task: Hadoop job information for Stage-2: number of mappers: 0; number of reducers: 0
2015-05-25 20:39:43,230 Stage-2 map = 0%,  reduce = 0%
15/05/25 20:39:43 INFO exec.Task: 2015-05-25 20:39:43,230 Stage-2 map = 0%,  reduce = 0%
15/05/25 20:39:43 INFO mapred.MapTask: Processing split: Paths:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002/000000_0:0+4235InputFormatClass: org.apache.hadoop.mapred.SequenceFileInputFormat

15/05/25 20:39:43 INFO io.HiveContextAwareRecordReader: Processing file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002/000000_0
15/05/25 20:39:43 INFO mapred.MapTask: numReduceTasks: 1
15/05/25 20:39:43 INFO mapred.MapTask: io.sort.mb = 100
15/05/25 20:39:43 INFO mapred.MapTask: data buffer = 79691776/99614720
15/05/25 20:39:43 INFO mapred.MapTask: record buffer = 262144/327680
15/05/25 20:39:43 INFO mr.ExecMapper: maximum memory = 922746880
15/05/25 20:39:43 INFO mr.ExecMapper: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO mr.ExecMapper: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO exec.MapOperator: Adding alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002 to work list for file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:43 INFO exec.MapOperator: dump TS struct<_col0:string,_col1:double>
15/05/25 20:39:43 INFO mr.ExecMapper: 
<MAP>Id =288
  <Children>
    <TS>Id =261
      <Children>
        <RS>Id =252
          <Parent>Id = 261 null<\Parent>
        <\RS>
      <\Children>
      <Parent>Id = 288 null<\Parent>
    <\TS>
  <\Children>
<\MAP>
15/05/25 20:39:43 INFO exec.MapOperator: Initializing Self 288 MAP
15/05/25 20:39:43 INFO exec.TableScanOperator: Initializing Self 261 TS
15/05/25 20:39:43 INFO exec.TableScanOperator: Operator 261 TS initialized
15/05/25 20:39:43 INFO exec.TableScanOperator: Initializing children of 261 TS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initializing child 252 RS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initializing Self 252 RS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Using tag = -1
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Operator 252 RS initialized
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initialization Done 252 RS
15/05/25 20:39:43 INFO exec.TableScanOperator: Initialization Done 261 TS
15/05/25 20:39:43 INFO exec.MapOperator: Initialization Done 288 MAP
15/05/25 20:39:43 INFO exec.MapOperator: Processing alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002 for file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002
15/05/25 20:39:43 INFO exec.MapOperator: 288 forwarding 1 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 forwarding 1 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 1 rows: used memory = 184288016
15/05/25 20:39:43 INFO exec.MapOperator: 288 forwarding 10 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 forwarding 10 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 10 rows: used memory = 184288016
15/05/25 20:39:43 INFO exec.MapOperator: 288 forwarding 100 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 forwarding 100 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 100 rows: used memory = 184288016
15/05/25 20:39:43 INFO exec.MapOperator: 288 finished. closing... 
15/05/25 20:39:43 INFO exec.MapOperator: 288 forwarded 112 rows
15/05/25 20:39:43 INFO exec.MapOperator: DESERIALIZE_ERRORS:0
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 finished. closing... 
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 forwarded 112 rows
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: 252 finished. closing... 
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: 252 forwarded 0 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 261 Close done
15/05/25 20:39:43 INFO exec.MapOperator: 288 Close done
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processed 112 rows: used memory = 185770136
15/05/25 20:39:43 INFO mapred.MapTask: Starting flush of map output
15/05/25 20:39:43 INFO mapred.MapTask: Finished spill 0
15/05/25 20:39:43 INFO mapred.Task: Task:attempt_local243227878_0016_m_000000_0 is done. And is in the process of commiting
15/05/25 20:39:43 INFO mapred.LocalJobRunner: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10002/000000_0:0+4235
15/05/25 20:39:43 INFO mapred.Task: Task 'attempt_local243227878_0016_m_000000_0' done.
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local243227878_0016_m_000000_0
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Map task executor complete.
15/05/25 20:39:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5196435d
15/05/25 20:39:43 INFO mapred.LocalJobRunner: 
15/05/25 20:39:43 INFO mapred.Merger: Merging 1 sorted segments
15/05/25 20:39:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3653 bytes
15/05/25 20:39:43 INFO mapred.LocalJobRunner: 
15/05/25 20:39:43 INFO ExecReducer: maximum memory = 922746880
15/05/25 20:39:43 INFO ExecReducer: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO ExecReducer: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO ExecReducer: 
<GBY>Id =253
  <Children>
    <SEL>Id =254
      <Children>
        <FS>Id =262
          <Parent>Id = 254 null<\Parent>
        <\FS>
      <\Children>
      <Parent>Id = 253 null<\Parent>
    <\SEL>
  <\Children>
<\GBY>
15/05/25 20:39:43 INFO exec.GroupByOperator: Initializing Self 253 GBY
15/05/25 20:39:43 INFO exec.GroupByOperator: Operator 253 GBY initialized
15/05/25 20:39:43 INFO exec.GroupByOperator: Initializing children of 253 GBY
15/05/25 20:39:43 INFO exec.SelectOperator: Initializing child 254 SEL
15/05/25 20:39:43 INFO exec.SelectOperator: Initializing Self 254 SEL
15/05/25 20:39:43 INFO exec.SelectOperator: SELECT struct<_col0:string,_col1:double>
15/05/25 20:39:43 INFO exec.SelectOperator: Operator 254 SEL initialized
15/05/25 20:39:43 INFO exec.SelectOperator: Initializing children of 254 SEL
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initializing child 262 FS
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initializing Self 262 FS
15/05/25 20:39:43 INFO exec.FileSinkOperator: Operator 262 FS initialized
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initialization Done 262 FS
15/05/25 20:39:43 INFO exec.SelectOperator: Initialization Done 254 SEL
15/05/25 20:39:43 INFO exec.GroupByOperator: Initialization Done 253 GBY
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 1 rows: used memory = 187227720
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 forwarding 1 rows
15/05/25 20:39:43 INFO exec.SelectOperator: 254 forwarding 1 rows
15/05/25 20:39:43 INFO exec.FileSinkOperator: Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10003/000000_0
15/05/25 20:39:43 INFO exec.FileSinkOperator: Writing to temp file: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_task_tmp.-mr-10003/_tmp.000000_0
15/05/25 20:39:43 INFO exec.FileSinkOperator: New Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10003/000000_0
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 10 rows: used memory = 188948232
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 forwarding 10 rows
15/05/25 20:39:43 INFO exec.SelectOperator: 254 forwarding 10 rows
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 100 rows: used memory = 190430352
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 forwarding 100 rows
15/05/25 20:39:43 INFO exec.SelectOperator: 254 forwarding 100 rows
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processed 112 rows: used memory = 190430352
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 finished. closing... 
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 forwarded 112 rows
15/05/25 20:39:43 INFO exec.SelectOperator: 254 finished. closing... 
15/05/25 20:39:43 INFO exec.SelectOperator: 254 forwarded 112 rows
15/05/25 20:39:43 INFO exec.FileSinkOperator: 262 finished. closing... 
15/05/25 20:39:43 INFO exec.FileSinkOperator: 262 forwarded 0 rows
15/05/25 20:39:43 INFO exec.SelectOperator: 254 Close done
15/05/25 20:39:43 INFO exec.GroupByOperator: 253 Close done
15/05/25 20:39:43 INFO mapred.Task: Task:attempt_local243227878_0016_r_000000_0 is done. And is in the process of commiting
15/05/25 20:39:43 INFO mapred.LocalJobRunner: reduce > reduce
15/05/25 20:39:43 INFO mapred.Task: Task 'attempt_local243227878_0016_r_000000_0' done.
2015-05-25 20:39:43,345 Stage-2 map = 0%,  reduce = 100%
15/05/25 20:39:43 INFO exec.Task: 2015-05-25 20:39:43,345 Stage-2 map = 0%,  reduce = 100%
Ended Job = job_local243227878_0016
15/05/25 20:39:43 INFO exec.Task: Ended Job = job_local243227878_0016
15/05/25 20:39:43 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10003 to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10003.intermediate
15/05/25 20:39:43 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-mr-10003.intermediate to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=task.MAPRED.Stage-2 start=1432597182948 end=1432597183357 duration=409>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=task.MAPRED.Stage-3>
Launching Job 3 out of 3
15/05/25 20:39:43 INFO ql.Driver: Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
15/05/25 20:39:43 INFO exec.Task: Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
15/05/25 20:39:43 INFO exec.Task: In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
15/05/25 20:39:43 INFO exec.Task:   set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
15/05/25 20:39:43 INFO exec.Task: In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
15/05/25 20:39:43 INFO exec.Task:   set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
15/05/25 20:39:43 INFO exec.Task: In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
15/05/25 20:39:43 INFO exec.Task:   set mapred.reduce.tasks=<number>
15/05/25 20:39:43 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:43 INFO mr.ExecDriver: Using org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
15/05/25 20:39:43 INFO exec.Utilities: Processing alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO exec.Utilities: Adding input file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO exec.Utilities: Content Summary not cached for file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO ql.Context: New scratch dir is file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1
15/05/25 20:39:43 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:43 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597183363 end=1432597183404 duration=41>
15/05/25 20:39:43 INFO exec.Utilities: <PERFLOG method=serializePlan>
15/05/25 20:39:43 INFO exec.Utilities: </PERFLOG method=serializePlan start=1432597183405 end=1432597183446 duration=41>
15/05/25 20:39:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: <PERFLOG method=getSplits>
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003; using filter path file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO mapred.FileInputFormat: Total input paths to process : 1
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: number of splits 1
15/05/25 20:39:43 INFO io.CombineHiveInputFormat: </PERFLOG method=getSplits start=1432597183515 end=1432597183518 duration=3>
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Creating map.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-8237505661299710560_1780592515_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327-work--3146295455879028115 with rwxr-xr-x
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/map.xml#map.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-8237505661299710560_1780592515_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/map.xml
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Creating reduce.xml in /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-2614827544639367208_1389612053_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327-work--3373253349065785511 with rwxr-xr-x
15/05/25 20:39:43 INFO filecache.TrackerDistributedCacheManager: Cached file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/reduce.xml#reduce.xml as /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-2614827544639367208_1389612053_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/reduce.xml
15/05/25 20:39:43 WARN mapred.LocalJobRunner: LocalJobRunner does not support symlinking into current working dir.
15/05/25 20:39:43 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-8237505661299710560_1780592515_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/map.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/map.xml
15/05/25 20:39:43 INFO mapred.TaskRunner: Creating symlink: /tmp/junit2102258791818644947/hadooptmp/mapred/local/archive/-2614827544639367208_1389612053_225590451/file/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10009/8b43ca5e-c5fe-4470-a85e-aad13fee9327/reduce.xml <- /tmp/junit2102258791818644947/hadooptmp/mapred/local/localRunner/reduce.xml
Starting Job = job_local1343266944_0017, Tracking URL = http://localhost:8080/
15/05/25 20:39:43 INFO exec.Task: Starting Job = job_local1343266944_0017, Tracking URL = http://localhost:8080/
Kill Command = NO_BIN! job  -kill job_local1343266944_0017
15/05/25 20:39:43 INFO exec.Task: Kill Command = NO_BIN! job  -kill job_local1343266944_0017
Hadoop job information for Stage-3: number of mappers: 0; number of reducers: 0
15/05/25 20:39:43 INFO exec.Task: Hadoop job information for Stage-3: number of mappers: 0; number of reducers: 0
2015-05-25 20:39:43,575 Stage-3 map = 0%,  reduce = 0%
15/05/25 20:39:43 INFO exec.Task: 2015-05-25 20:39:43,575 Stage-3 map = 0%,  reduce = 0%
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Waiting for map tasks
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Starting task: attempt_local1343266944_0017_m_000000_0
15/05/25 20:39:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@1f9985cb
15/05/25 20:39:43 INFO mapred.MapTask: Processing split: Paths:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003/000000_0:0+4235InputFormatClass: org.apache.hadoop.mapred.SequenceFileInputFormat

15/05/25 20:39:43 INFO io.HiveContextAwareRecordReader: Processing file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003/000000_0
15/05/25 20:39:43 INFO mapred.MapTask: numReduceTasks: 1
15/05/25 20:39:43 INFO mapred.MapTask: io.sort.mb = 100
15/05/25 20:39:43 INFO mapred.MapTask: data buffer = 79691776/99614720
15/05/25 20:39:43 INFO mapred.MapTask: record buffer = 262144/327680
15/05/25 20:39:43 INFO mr.ExecMapper: maximum memory = 922746880
15/05/25 20:39:43 INFO mr.ExecMapper: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO mr.ExecMapper: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO exec.MapOperator: Adding alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003 to work list for file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO exec.MapOperator: dump TS struct<_col0:string,_col1:double>
15/05/25 20:39:43 INFO mr.ExecMapper: 
<MAP>Id =296
  <Children>
    <TS>Id =263
      <Children>
        <RS>Id =255
          <Parent>Id = 263 null<\Parent>
        <\RS>
      <\Children>
      <Parent>Id = 296 null<\Parent>
    <\TS>
  <\Children>
<\MAP>
15/05/25 20:39:43 INFO exec.MapOperator: Initializing Self 296 MAP
15/05/25 20:39:43 INFO exec.TableScanOperator: Initializing Self 263 TS
15/05/25 20:39:43 INFO exec.TableScanOperator: Operator 263 TS initialized
15/05/25 20:39:43 INFO exec.TableScanOperator: Initializing children of 263 TS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initializing child 255 RS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initializing Self 255 RS
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Using tag = -1
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Operator 255 RS initialized
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: Initialization Done 255 RS
15/05/25 20:39:43 INFO exec.TableScanOperator: Initialization Done 263 TS
15/05/25 20:39:43 INFO exec.MapOperator: Initialization Done 296 MAP
15/05/25 20:39:43 INFO exec.MapOperator: Processing alias file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003 for file file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003
15/05/25 20:39:43 INFO exec.MapOperator: 296 forwarding 1 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 forwarding 1 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 1 rows: used memory = 324472208
15/05/25 20:39:43 INFO exec.MapOperator: 296 forwarding 10 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 forwarding 10 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 10 rows: used memory = 324472208
15/05/25 20:39:43 INFO exec.MapOperator: 296 forwarding 100 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 forwarding 100 rows
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processing 100 rows: used memory = 324472208
15/05/25 20:39:43 INFO exec.MapOperator: 296 finished. closing... 
15/05/25 20:39:43 INFO exec.MapOperator: 296 forwarded 112 rows
15/05/25 20:39:43 INFO exec.MapOperator: DESERIALIZE_ERRORS:0
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 finished. closing... 
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 forwarded 112 rows
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: 255 finished. closing... 
15/05/25 20:39:43 INFO exec.ReduceSinkOperator: 255 forwarded 0 rows
15/05/25 20:39:43 INFO exec.TableScanOperator: 263 Close done
15/05/25 20:39:43 INFO exec.MapOperator: 296 Close done
15/05/25 20:39:43 INFO mr.ExecMapper: ExecMapper: processed 112 rows: used memory = 325954328
15/05/25 20:39:43 INFO mapred.MapTask: Starting flush of map output
15/05/25 20:39:43 INFO mapred.MapTask: Finished spill 0
15/05/25 20:39:43 INFO mapred.Task: Task:attempt_local1343266944_0017_m_000000_0 is done. And is in the process of commiting
15/05/25 20:39:43 INFO mapred.LocalJobRunner: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-mr-10003/000000_0:0+4235
15/05/25 20:39:43 INFO mapred.Task: Task 'attempt_local1343266944_0017_m_000000_0' done.
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local1343266944_0017_m_000000_0
15/05/25 20:39:43 INFO mapred.LocalJobRunner: Map task executor complete.
15/05/25 20:39:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@17d9a235
15/05/25 20:39:43 INFO mapred.LocalJobRunner: 
15/05/25 20:39:43 INFO mapred.Merger: Merging 1 sorted segments
15/05/25 20:39:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4549 bytes
15/05/25 20:39:43 INFO mapred.LocalJobRunner: 
15/05/25 20:39:43 INFO ExecReducer: maximum memory = 922746880
15/05/25 20:39:43 INFO ExecReducer: conf classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO ExecReducer: thread classpath = [file:/home/usuario/workspace/testsHiveHadoop/target/surefire/surefirebooter8785722590624911197.jar]
15/05/25 20:39:43 INFO ExecReducer: 
<EX>Id =256
  <Children>
    <LIM>Id =257
      <Children>
        <FS>Id =258
          <Parent>Id = 257 null<\Parent>
        <\FS>
      <\Children>
      <Parent>Id = 256 null<\Parent>
    <\LIM>
  <\Children>
<\EX>
15/05/25 20:39:43 INFO exec.ExtractOperator: Initializing Self 256 EX
15/05/25 20:39:43 INFO exec.ExtractOperator: Operator 256 EX initialized
15/05/25 20:39:43 INFO exec.ExtractOperator: Initializing children of 256 EX
15/05/25 20:39:43 INFO exec.LimitOperator: Initializing child 257 LIM
15/05/25 20:39:43 INFO exec.LimitOperator: Initializing Self 257 LIM
15/05/25 20:39:43 INFO exec.LimitOperator: Operator 257 LIM initialized
15/05/25 20:39:43 INFO exec.LimitOperator: Initializing children of 257 LIM
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initializing child 258 FS
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initializing Self 258 FS
15/05/25 20:39:43 INFO exec.FileSinkOperator: Operator 258 FS initialized
15/05/25 20:39:43 INFO exec.FileSinkOperator: Initialization Done 258 FS
15/05/25 20:39:43 INFO exec.LimitOperator: Initialization Done 257 LIM
15/05/25 20:39:43 INFO exec.ExtractOperator: Initialization Done 256 EX
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 1 rows: used memory = 327411912
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 forwarding 1 rows
15/05/25 20:39:43 INFO exec.LimitOperator: 257 forwarding 1 rows
15/05/25 20:39:43 INFO exec.FileSinkOperator: Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-ext-10001/000000_0
15/05/25 20:39:43 INFO exec.FileSinkOperator: Writing to temp file: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_task_tmp.-ext-10001/_tmp.000000_0
15/05/25 20:39:43 INFO exec.FileSinkOperator: New Final Path: FS file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-ext-10001/000000_0
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 10 rows: used memory = 327411912
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 forwarding 10 rows
15/05/25 20:39:43 INFO exec.LimitOperator: 257 forwarding 10 rows
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processing 100 rows: used memory = 328869488
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 forwarding 100 rows
15/05/25 20:39:43 INFO exec.LimitOperator: 257 forwarding 100 rows
15/05/25 20:39:43 INFO ExecReducer: ExecReducer: processed 112 rows: used memory = 328869488
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 finished. closing... 
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 forwarded 112 rows
15/05/25 20:39:43 INFO exec.LimitOperator: 257 finished. closing... 
15/05/25 20:39:43 INFO exec.LimitOperator: 257 forwarded 112 rows
15/05/25 20:39:43 INFO exec.FileSinkOperator: 258 finished. closing... 
15/05/25 20:39:43 INFO exec.FileSinkOperator: 258 forwarded 0 rows
15/05/25 20:39:43 INFO exec.LimitOperator: 257 Close done
15/05/25 20:39:43 INFO exec.ExtractOperator: 256 Close done
15/05/25 20:39:43 INFO mapred.Task: Task:attempt_local1343266944_0017_r_000000_0 is done. And is in the process of commiting
15/05/25 20:39:43 INFO mapred.LocalJobRunner: reduce > reduce
15/05/25 20:39:43 INFO mapred.Task: Task 'attempt_local1343266944_0017_r_000000_0' done.
2015-05-25 20:39:43,645 Stage-3 map = 0%,  reduce = 100%
15/05/25 20:39:43 INFO exec.Task: 2015-05-25 20:39:43,645 Stage-3 map = 0%,  reduce = 100%
Ended Job = job_local1343266944_0017
15/05/25 20:39:43 INFO exec.Task: Ended Job = job_local1343266944_0017
15/05/25 20:39:43 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-ext-10001 to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-ext-10001.intermediate
15/05/25 20:39:43 INFO exec.FileSinkOperator: Moving tmp dir: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/_tmp.-ext-10001.intermediate to: file:/tmp/junit2102258791818644947/scratchdir/hive_2015-05-25_20-39-42_107_7773497072626393114-1/-ext-10001
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=task.MAPRED.Stage-3 start=1432597183357 end=1432597183650 duration=293>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=runTasks start=1432597182300 end=1432597183650 duration=1350>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597182299 end=1432597183650 duration=1351>
MapReduce Jobs Launched: 
15/05/25 20:39:43 INFO ql.Driver: MapReduce Jobs Launched: 
Job 0:  HDFS Read: 0 HDFS Write: 0 SUCCESS
15/05/25 20:39:43 INFO ql.Driver: Job 0:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
15/05/25 20:39:43 INFO ql.Driver: Job 1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Job 2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
15/05/25 20:39:43 INFO ql.Driver: Job 2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
15/05/25 20:39:43 INFO ql.Driver: Total MapReduce CPU Time Spent: 0 msec
OK
15/05/25 20:39:43 INFO ql.Driver: OK
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597183657 end=1432597183657 duration=0>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597182106 end=1432597183657 duration=1551>
15/05/25 20:39:43 INFO mapred.FileInputFormat: Total input paths to process : 1
15/05/25 20:39:43 ERROR hiverunner.StandaloneHiveRunner: expected:<[  	256.98, (Rx)  	261.55, 24	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.808, Amlodipine  	24.094, Amoxicillin  	238.944, Amphetamine  	56.779, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]> but was:<[  	256.97999999999996, (Rx)  	261.55, 24  	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.80799999999999, Amlodipine  	24.094, Amoxicillin  	238.94400000000002, Amphetamine  	56.778999999999996, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]>
java.lang.AssertionError: expected:<[  	256.98, (Rx)  	261.55, 24	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.808, Amlodipine  	24.094, Amoxicillin  	238.944, Amphetamine  	56.779, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]> but was:<[  	256.97999999999996, (Rx)  	261.55, 24  	48.663, AC  	45.209, APAP  	194.228, Abilify  	29.704, Acetonide  	56.972, Actos  	14.218, Advair  	16.462, Albuterol  	122.833, Alendronate  	44.834, Alprazolam  	38.075, Amitriptyline  	63.80799999999999, Amlodipine  	24.094, Amoxicillin  	238.94400000000002, Amphetamine  	56.778999999999996, Atenolol  	162.136, Azithromycin  	86.009, Benicar  	33.397]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at com.bdicm.app.TestCasesSprint_01.testQuery_09(TestCasesSprint_01.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at com.klarna.hiverunner.StandaloneHiveRunner.evaluateStatement(StandaloneHiveRunner.java:100)
	at com.klarna.hiverunner.StandaloneHiveRunner.access$000(StandaloneHiveRunner.java:53)
	at com.klarna.hiverunner.StandaloneHiveRunner$1$1.evaluate(StandaloneHiveRunner.java:79)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
15/05/25 20:39:43 INFO service.HiveServer: Running the query: USE default
15/05/25 20:39:43 INFO exec.ListSinkOperator: 264 finished. closing... 
15/05/25 20:39:43 INFO exec.ListSinkOperator: 264 forwarded 0 rows
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=Driver.run>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=TimeToSubmit>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=compile>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=parse>
15/05/25 20:39:43 INFO parse.ParseDriver: Parsing command: USE default
15/05/25 20:39:43 INFO parse.ParseDriver: Parse Completed
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=parse start=1432597183662 end=1432597183663 duration=1>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=semanticAnalyze>
15/05/25 20:39:43 INFO ql.Driver: Semantic Analysis Completed
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=semanticAnalyze start=1432597183663 end=1432597183663 duration=0>
15/05/25 20:39:43 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=compile start=1432597183662 end=1432597183664 duration=2>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=Driver.execute>
15/05/25 20:39:43 INFO ql.Driver: Starting command: USE default
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=TimeToSubmit start=1432597183662 end=1432597183664 duration=2>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=runTasks>
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=task.DDL.Stage-0>
15/05/25 20:39:43 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:43 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:43 INFO metastore.HiveMetaStore: 0: get_database: default
15/05/25 20:39:43 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=task.DDL.Stage-0 start=1432597183664 end=1432597183678 duration=14>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=runTasks start=1432597183664 end=1432597183678 duration=14>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=Driver.execute start=1432597183664 end=1432597183678 duration=14>
OK
15/05/25 20:39:43 INFO ql.Driver: OK
15/05/25 20:39:43 INFO ql.Driver: <PERFLOG method=releaseLocks>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=releaseLocks start=1432597183679 end=1432597183679 duration=0>
15/05/25 20:39:43 INFO ql.Driver: </PERFLOG method=Driver.run start=1432597183662 end=1432597183679 duration=17>
15/05/25 20:39:43 INFO metastore.HiveMetaStore: 0: Shutting down the object store...
15/05/25 20:39:43 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
15/05/25 20:39:43 INFO metastore.HiveMetaStore: 0: Metastore shutdown complete.
15/05/25 20:39:43 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
15/05/25 20:39:43 INFO hiverunner.HiveServerContainer: Tore down HiveServer instance
]]></system-err>
  </testcase>
  <testcase name="testTablesCreated" classname="com.bdicm.app.TestCasesSprint_01" time="0.701"/>
</testsuite>